spark-bench = {
  spark-submit-config = [
    {
      conf = {
//        "spark.driver.extraJavaOptions" = "-Xrunjdwp:transport=dt_socket,server=n,address=localhost:5005,suspend=y,onuncaught=n"
        "spark.ui.showConsoleProgress" = "false"
        "spark.ui.enabled" = "false"
        "spark.ui.retainedJobs" = "0"
        "spark.ui.retainedStages" = "0"
        "spark.ui.retainedTasks" = "0"
        "spark.sql.ui.retainedExecutions" = "0"
        "spark.sql.autoBroadcastJoinThreshold" = "-1"
        "spark.sql.crossJoin.enabled" = "true"
        "spark.serializer" = "org.apache.spark.serializer.KryoSerializer"
        "spark.dynamicAllocation.enabled" = "false"
        "spark.shuffle.service.enabled" = "false"
        "spark.debug.maxToStringFields" = "100"
        "spark.sql.warehouse.dir" = "hdfs:///tpcds-warehouse-partitioned-1GB"
        "spark.executor.cores" = "1"
        "spark.max.cores" = "54"
        "spark.executor.memory" = "2G"
        "spark.driver.memory" = "1G"
        "spark.app.name" = "tpcds-gen-1GB"
        "spark.driver.maxResultSize" = "1G"
      }
      suites-parallel = false
      enable-hive = true
      workload-suites = [
        {
          descr = "Generate a partitioned 1G dataset and queries"
          benchmark-output = "hdfs:///ss-partitioned-gen-1GB.csv"
          parallel = false
          workloads = [
            {
              name = "tpcdsdatagen"
              tpcds-data-output = "hdfs:///tpcds-data-partitioned-1GB"
              clean = false
              tpcds-kit-dir = "hdfs:///tpcds-kit"
              table-options = "hdfs:///table-options-partitioned-1GB.json"
              tpcds-scale = 1
              tpcds-dsdgen-validate = true
            },
            {
              name = "tpcdsquerygen"
              tpcds-query-output = "hdfs:///tpcds-queries-partitioned-1GB"
              tpcds-kit-dir = "hdfs:///tpcds-kit"
              tpcds-scale = 1
              tpcds-streams = 1
            }
          ]
        }
      ]
    }
  ]
}
