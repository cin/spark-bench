spark-bench = {
  spark-submit-config = [
    {
      spark-bench-jar = "hdfs://hdfs:9000/spark-bench/lib/spark-bench-2.3.0_0.4-SNAPSHOT.jar"
      conf = {
        "spark.ui.showConsoleProgress" = "false"
        "spark.ui.enabled" = "false"
        "spark.ui.retainedJobs" = "0"
        "spark.ui.retainedStages" = "0"
        "spark.ui.retainedTasks" = "0"
        "spark.sql.ui.retainedExecutions" = "0"
        "spark.sql.autoBroadcastJoinThreshold" = "134217728"
        "spark.sql.crossJoin.enabled" = "true"
        "spark.sql.shuffle.partitions" = "200"
        "spark.serializer" = "org.apache.spark.serializer.KryoSerializer"
        "spark.debug.maxToStringFields" = "100"
        "spark.sql.warehouse.dir" = "hdfs://hdfs:9000/tpcds-warehouse-partitioned-1GB"
        "spark.executor.cores" = "1"
        "spark.executor.instances" = "38"
        "spark.executor.memory" = "6G"
        "spark.driver.memory" = "6G"
        "spark.app.name" = "tpcds-partitioned-query-1GB"
        "spark.driver.maxResultSize" = "4G"
        "spark.kubernetes.container.image" = "showermat/spark:2.3.0-hadoop3.0.0"
        "spark.kubernetes.allocation.batch.size" = "20"
        "spark.jars" = "hdfs://hdfs:9000/spark-bench/lib/spark-bench-2.3.0_0.4-SNAPSHOT.jar"
      }
      suites-parallel = false
      workload-suites = [
        {
          descr = "Query a partitioned 1GB dataset"
          benchmark-output = "hdfs://hdfs:9000/sok-partitioned-bench-output.csv"
          save-mode = "append"
          parallel = false
          repeat = 1
          workloads = [
            {
              name = "tpcds"
              querystream = "hdfs://hdfs:9000/tpcds-queries-partitioned-1GB/query_0.sql"
              createtemptables = true
              explainqueries = true
            }
          ]
        }
      ]
    }
  ]
}
